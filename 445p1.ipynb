{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EECS 445 - Winter 2018\n",
    "# Project 1 - project1.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import string\n",
    "import random\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_data(fname):\n",
    "    \"\"\"\n",
    "    Reads in a csv file and return a dataframe. A dataframe df is similar to dictionary.\n",
    "    You can access the label by calling df['label'], the content by df['content']\n",
    "    the rating by df['rating']\n",
    "    \"\"\"\n",
    "    return pd.read_csv(fname)\n",
    "\n",
    "\n",
    "def get_split_binary_data():\n",
    "    \"\"\"\n",
    "    Reads in the data from data/dataset.csv and returns it using\n",
    "    extract_dictionary and generate_feature_matrix split into training and test sets.\n",
    "    The binary labels take two values:\n",
    "        -1: poor/average\n",
    "         1: good\n",
    "    Also returns the dictionary used to create the feature matrices.\n",
    "    \"\"\"\n",
    "    fname = \"data/dataset.csv\"\n",
    "    dataframe = load_data(fname)\n",
    "    dataframe = dataframe[dataframe['label'] != 0]\n",
    "    positiveDF = dataframe[dataframe['label'] == 1].copy()\n",
    "    negativeDF = dataframe[dataframe['label'] == -1].copy()\n",
    "    X_train = pd.concat([positiveDF[:500], negativeDF[:500]]).reset_index(drop=True).copy()\n",
    "    dictionary = extract_dictionary(X_train)\n",
    "    X_test = pd.concat([positiveDF[500:700], negativeDF[500:700]]).reset_index(drop=True).copy()\n",
    "    Y_train = X_train['label'].values.copy()\n",
    "    Y_test = X_test['label'].values.copy()\n",
    "    X_train = generate_feature_matrix(X_train, dictionary)\n",
    "    X_test = generate_feature_matrix(X_test, dictionary)\n",
    "\n",
    "    return (X_train, Y_train, X_test, Y_test, dictionary)\n",
    "\n",
    "\n",
    "def get_imbalanced_data(dictionary):\n",
    "    \"\"\"\n",
    "    Reads in the data from data/imbalanced.csv and returns it using\n",
    "    extract_dictionary and generate_feature_matrix as a tuple\n",
    "    (X_train, Y_train) where the labels are binary as follows\n",
    "        -1: poor/average\n",
    "        1: good\n",
    "    Input:\n",
    "        dictionary: the dictionary created via get_split_binary_data\n",
    "    \"\"\"\n",
    "    fname = \"data/imbalanced.csv\"\n",
    "    dataframe = load_data(fname)\n",
    "    dataframe = dataframe[dataframe['label'] != 0]\n",
    "    positiveDF = dataframe[dataframe['label'] == 1].copy()\n",
    "    negativeDF = dataframe[dataframe['label'] == -1].copy()\n",
    "    dataframe = pd.concat([positiveDF[:300], negativeDF[:700]]).reset_index(drop=True).copy()\n",
    "    X_train = generate_feature_matrix(dataframe, dictionary)\n",
    "    Y_train = dataframe['label'].values.copy()\n",
    "\n",
    "    return (X_train, Y_train)\n",
    "\n",
    "\n",
    "def get_imbalanced_test(dictionary):\n",
    "    \"\"\"\n",
    "    Reads in the data from data/dataset.csv and returns a subset of it\n",
    "    reflecting an imbalanced test dataset\n",
    "        -1: poor/average\n",
    "        1: good\n",
    "    Input:\n",
    "        dictionary: the dictionary created via get_split_binary_data\n",
    "    \"\"\"\n",
    "    fname = \"data/dataset.csv\"\n",
    "    dataframe = load_data(fname)\n",
    "    dataframe = dataframe[dataframe['label'] != 0]\n",
    "    positiveDF = dataframe[dataframe['label'] == 1].copy()\n",
    "    negativeDF = dataframe[dataframe['label'] == -1].copy()\n",
    "    X_test = pd.concat([positiveDF[:400], negativeDF[:100]]).reset_index(drop=True).copy()\n",
    "    Y_test = X_test['label'].values.copy()\n",
    "    X_test = generate_feature_matrix(X_test, dictionary)\n",
    "\n",
    "    return (X_test, Y_test)\n",
    "\n",
    "\n",
    "def get_multiclass_training_data():\n",
    "    \"\"\"\n",
    "    Reads in the data from data/dataset.csv and returns it using\n",
    "    extract_dictionary and generate_feature_matrix as a tuple\n",
    "    (X_train, Y_train) where the labels are multiclass as follows\n",
    "        -1: poor\n",
    "         0: average\n",
    "         1: good\n",
    "    Also returns the dictionary used to create X_train.\n",
    "    \"\"\"\n",
    "    fname = \"data/dataset.csv\"\n",
    "    dataframe = load_data(fname)\n",
    "    dictionary = extract_dictionary(dataframe)\n",
    "    X_train = generate_feature_matrix(dataframe, dictionary)\n",
    "    Y_train = dataframe['label'].values.copy()\n",
    "\n",
    "    return (X_train, Y_train, dictionary)\n",
    "\n",
    "\n",
    "def get_heldout_reviews(dictionary):\n",
    "    \"\"\"\n",
    "    Reads in the data from data/heldout.csv and returns it as a feature\n",
    "    matrix based on the functions extract_dictionary and generate_feature_matrix\n",
    "    Input:\n",
    "        dictionary: the dictionary created by get_multiclass_training_data\n",
    "    \"\"\"\n",
    "    fname = \"data/heldout.csv\"\n",
    "    dataframe = load_data(fname)\n",
    "    X = generate_feature_matrix(dataframe, dictionary)\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_challenge_labels(y, uniqname):\n",
    "    \"\"\"\n",
    "    Takes in a numpy array that stores the prediction of your multiclass\n",
    "    classifier and output the prediction to held_out_result.csv. Please make sure that\n",
    "    you do not change the order of the ratings in the heldout dataset since we will\n",
    "    this file to evaluate your classifier.\n",
    "    \"\"\"\n",
    "    pd.Series(np.array(y)).to_csv(uniqname+'.csv', header=['label'], index=False)\n",
    "    return\n",
    "\n",
    "def select_classifier(penalty='l2', c=1.0, degree=1, r=0.0, class_weight='balanced'):\n",
    "    \"\"\"\n",
    "        Return a linear svm classifier based on the given\n",
    "        penalty function and regularization parameter c.\n",
    "        \"\"\"\n",
    "    # TODO: Optionally implement this helper function if you would like to\n",
    "    # instantiate your SVM classifiers in a single function. You will need\n",
    "    # to use the above parameters throughout the assignment.\n",
    "    if penalty==\"l2\":\n",
    "        if degree==1:\n",
    "            return SVC(kernel=\"linear\", C=c, degree=degree, coef0=r, class_weight=class_weight)\n",
    "        return SVC(kernel=\"poly\", C=c, degree=degree, gamma=\"auto\",coef0=r, class_weight=class_weight)\n",
    "    if penalty==\"l1\":\n",
    "        return LinearSVC(penalty=\"l1\", C=c, class_weight=class_weight, dual=False, max_iter=1000)\n",
    "\n",
    "\n",
    "def extract_dictionary(df):\n",
    "    \"\"\"\n",
    "        Reads a panda dataframe, and returns a dictionary of distinct words\n",
    "        mapping from each distinct word to its index (ordered by when it was found).\n",
    "        Input:\n",
    "        df: dataframe/output of load_data()\n",
    "        Returns:\n",
    "        a dictionary of distinct words that maps each distinct word\n",
    "        to a unique index corresponding to when it was first found while\n",
    "        iterating over all words in each review in the dataframe df\n",
    "        \"\"\"\n",
    "    word_dict = {}\n",
    "    \n",
    "    # TODO: Implement this function\n",
    "    index=0\n",
    "    for text in df[\"text\"]:\n",
    "        for p in string.punctuation:\n",
    "            text=text.replace(p,\" \")\n",
    "        text=text.lower()\n",
    "        spl=text.split()\n",
    "        for word in spl:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word]=index\n",
    "                index=index+1\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def generate_feature_matrix(df, word_dict):\n",
    "    \"\"\"\n",
    "        Reads a dataframe and the dictionary of unique words\n",
    "        to generate a matrix of {1, 0} feature vectors for each review.\n",
    "        Use the word_dict to find the correct index to set to 1 for each place\n",
    "        in the feature vector. The resulting feature matrix should be of\n",
    "        dimension (number of reviews, number of words).\n",
    "        Input:\n",
    "        df: dataframe that has the ratings and labels\n",
    "        word_list: dictionary of words mapping to indices\n",
    "        Returns:\n",
    "        a feature matrix of dimension (number of reviews, number of words)\n",
    "        \"\"\"\n",
    "    number_of_reviews = df.shape[0]\n",
    "    number_of_words = len(word_dict)\n",
    "    feature_matrix = np.zeros((number_of_reviews, number_of_words))\n",
    "    # TODO: Implement this function\n",
    "    index=0\n",
    "    for text in df[\"text\"]:\n",
    "        for p in string.punctuation:\n",
    "            text=text.replace(p,\" \")\n",
    "        text=text.lower()\n",
    "        spl=text.split()\n",
    "        for word in spl:\n",
    "            if word in word_dict:\n",
    "                feature_matrix[index,word_dict[word]]=1\n",
    "        index=index+1\n",
    "    return feature_matrix\n",
    "\n",
    "def cv_performance(clf, X, y, k=5, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "        Splits the data X and the labels y into k-folds and runs k-fold\n",
    "        cross-validation: for each fold i in 1...k, trains a classifier on\n",
    "        all the data except the ith fold, and tests on the ith fold.\n",
    "        Calculates the k-fold cross-validation performance metric for classifier\n",
    "        clf by averaging the performance across folds.\n",
    "        Input:\n",
    "        clf: an instance of SVC()\n",
    "        X: (n,d) array of feature vectors, where n is the number of examples\n",
    "        and d is the number of features\n",
    "        y: (n,) array of binary labels {1,-1}\n",
    "        k: an int specifying the number of folds (default=5)\n",
    "        metric: string specifying the performance metric (default='accuracy'\n",
    "        other options: 'f1-score', 'auroc', 'precision', 'sensitivity',\n",
    "        and 'specificity')\n",
    "        Returns:\n",
    "        average 'test' performance across the k folds as np.float64\n",
    "        \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    scores = []\n",
    "    #HINT: You may find the StratifiedKFold from sklearn.model_selection\n",
    "    #to be useful\n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "    skf.get_n_splits(X,y)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        if metric==\"auroc\":\n",
    "            y_pred=clf.decision_function(X_test)\n",
    "        score=performance(y_test, y_pred, metric)\n",
    "        scores.append(score)\n",
    "    \n",
    "    #And return the average performance across all fold splits.\n",
    "    return np.array(scores).mean()\n",
    "\n",
    "\n",
    "def select_param_linear(X, y, k=5, metric=\"accuracy\", C_range = [], penalty='l2'):\n",
    "    \"\"\"\n",
    "        Sweeps different settings for the hyperparameter of a linear-kernel SVM,\n",
    "        calculating the k-fold CV performance for each setting on X, y.\n",
    "        Input:\n",
    "        X: (n,d) array of feature vectors, where n is the number of examples\n",
    "        and d is the number of features\n",
    "        y: (n,) array of binary labels {1,-1}\n",
    "        k: int specifying the number of folds (default=5)\n",
    "        metric: string specifying the performance metric (default='accuracy',\n",
    "        other options: 'f1-score', 'auroc', 'precision', 'sensitivity',\n",
    "        and 'specificity')\n",
    "        C_range: an array with C values to be searched over\n",
    "        Returns:\n",
    "        The parameter value for a linear-kernel SVM that maximizes the\n",
    "        average 5-fold CV performance.\n",
    "        \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    #HINT: You should be using your cv_performance function here\n",
    "    #to evaluate the performance of each SVM\n",
    "    maxc=0\n",
    "    maxperf=0\n",
    "    for c in C_range:\n",
    "        clf = select_classifier(c=c)\n",
    "        perf = cv_performance(clf, X, y, k=5, metric=metric)\n",
    "        print(c, perf)\n",
    "        if perf>maxperf:\n",
    "            maxperf=perf\n",
    "            maxc=c\n",
    "    return maxc\n",
    "\n",
    "\n",
    "def plot_weight(X,y,penalty,metric=\"\",C_range=[]):\n",
    "    \"\"\"\n",
    "        Takes as input the training data X and labels y and plots the L0-norm\n",
    "        (number of nonzero elements) of the coefficients learned by a classifier\n",
    "        as a function of the C-values of the classifier.\n",
    "        \"\"\"\n",
    "    \n",
    "    print(\"Plotting the number of nonzero entries of the parameter vector as a function of C\")\n",
    "    norm0 = []\n",
    "    \n",
    "    # TODO: Implement this part of the function\n",
    "    #Here, for each value of c in C_range, you should\n",
    "    #append to norm0 the L0-norm of the theta vector that is learned\n",
    "    #when fitting an L2- or L1-penalty, degree=1 SVM to the data (X, y)\n",
    "    \n",
    "    for c in C_range:\n",
    "        clf = select_classifier(penalty=penalty, c=c)\n",
    "        clf.fit(X,y)\n",
    "        n0=0\n",
    "        for theta in clf.coef_:\n",
    "            for c in theta:\n",
    "                if c!=0:\n",
    "                    n0+=1\n",
    "        norm0.append(n0)\n",
    "    \n",
    "    #This code will plot your L0-norm as a function of c\n",
    "    plt.plot(C_range, norm0)\n",
    "    plt.xscale('log')\n",
    "    plt.legend(['L0-norm'])\n",
    "    plt.xlabel(\"Value of C\")\n",
    "    plt.ylabel(\"Norm of theta\")\n",
    "    plt.title('Norm-'+penalty+'_penalty.png')\n",
    "    plt.savefig('Norm-'+penalty+'_penalty.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def select_param_quadratic(X, y, k=5, metric=\"accuracy\", param_range=[]):\n",
    "    \"\"\"\n",
    "        Sweeps different settings for the hyperparameters of an quadratic-kernel SVM,\n",
    "        calculating the k-fold CV performance for each setting on X, y.\n",
    "        Input:\n",
    "        X: (n,d) array of feature vectors, where n is the number of examples\n",
    "        and d is the number of features\n",
    "        y: (n,) array of binary labels {1,-1}\n",
    "        k: an int specifying the number of folds (default=5)\n",
    "        metric: string specifying the performance metric (default='accuracy'\n",
    "        other options: 'f1-score', 'auroc', 'precision', 'sensitivity',\n",
    "        and 'specificity')\n",
    "        parameter_values: a (num_param, 2)-sized array containing the\n",
    "        parameter values to search over. The first column should\n",
    "        represent the values for C, and the second column should\n",
    "        represent the values for r. Each row of this array thus\n",
    "        represents a pair of parameters to be tried together.\n",
    "        Returns:\n",
    "        The parameter value(s) for a quadratic-kernel SVM that maximize\n",
    "        the average 5-fold CV performance\n",
    "        \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Hint: This will be very similar to select_param_linear, except\n",
    "    # the type of SVM model you are using will be different...\n",
    "    \n",
    "    maxc=0\n",
    "    macr=0\n",
    "    maxperf=0\n",
    "    for c,r in param_range:\n",
    "        clf = select_classifier(c=c, degree=2, r=r)\n",
    "        #clf = SVC(kernel = 'poly', gamma = 'auto',degree = 2, C = c, coef0 = r, class_weight = 'balanced')\n",
    "        perf=cv_performance(clf, X, y, k=k, metric=metric)\n",
    "        print(c,r,perf)\n",
    "        if perf>maxperf:\n",
    "            maxc=c\n",
    "            maxr=r\n",
    "            maxperf=perf\n",
    "    return maxc, maxr\n",
    "\n",
    "\n",
    "def performance(y_true, y_pred, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "        Calculates the performance metric as evaluated on the true labels\n",
    "        y_true versus the predicted labels y_pred.\n",
    "        Input:\n",
    "        y_true: (n,) array containing known labels\n",
    "        y_pred: (n,) array containing predicted scores\n",
    "        metric: string specifying the performance metric (default='accuracy'\n",
    "        other options: 'f1-score', 'auroc', 'precision', 'sensitivity',\n",
    "        and 'specificity')\n",
    "        Returns:\n",
    "        the performance as an np.float64\n",
    "        \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # This is an optional but very useful function to implement.\n",
    "    # See the sklearn.metrics documentation for pointers on how to implement\n",
    "    # the requested metrics.\n",
    "\n",
    "    if metric==\"auroc\":\n",
    "        return metrics.roc_auc_score(y_true,y_pred)\n",
    "    m=metrics.confusion_matrix(y_true,y_pred)\n",
    "    tn, fp, fn, tp = m.ravel()\n",
    "    if metric==\"accuracy\":\n",
    "        return (tp+tn)/(tp+fn+fp+tn)\n",
    "    if metric==\"f1-score\":\n",
    "        pre=tp/(tp+fp)\n",
    "        sen=tp/(tp+fn)\n",
    "        return 2*pre*sen/(pre+sen)\n",
    "    if metric==\"sensitivity\":\n",
    "        return tp/(tp+fn)\n",
    "    if metric==\"precision\":\n",
    "        return tp/(tp+fp)\n",
    "    if metric==\"specificity\":\n",
    "        return tn/(tn+fp)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Read binary data\n",
    "    # NOTE: READING IN THE DATA WILL NOT WORK UNTIL YOU HAVE FINISHED\n",
    "    #       IMPLEMENTING generate_feature_matrix AND extract_dictionary\n",
    "    X_train, Y_train, X_test, Y_test, dictionary_binary = get_split_binary_data()\n",
    "    IMB_features, IMB_labels = get_imbalanced_data(dictionary_binary)\n",
    "    IMB_test_features, IMB_test_labels = get_imbalanced_test(dictionary_binary)\n",
    "    \n",
    "    # TODO: Questions 2, 3, 4\n",
    "    # average count of non-zero features\n",
    "    \n",
    "    # q2\n",
    "    print(\"Number of unique words:\",len(X_train[0]))\n",
    "    print(\"Average number of non-zero features:\",np.sum(X_train)/len(X_train))\n",
    "    \n",
    "    #q3.1(c)\n",
    "    metrics=[\"accuracy\",\"f1-score\",\"auroc\",\"precision\",\"sensitivity\",\"specificity\"]\n",
    "    selected_C=0\n",
    "    C_range=[1e-3, 1e-2, 0.1,1,10,100,1000]\n",
    "    for me in metrics:\n",
    "        maxc=select_param_linear(X_train, y_train, metric=me, C_range = C_range)\n",
    "        clf=select_classifier(c=maxc)\n",
    "        score=cv_performance(clf, X_train, y_train, metric=me)\n",
    "        print(\"C=\",maxc,\"is optimal under\",me,\"metric, cv_perf=\",score)\n",
    "        if me==\"auroc\":\n",
    "            selected_C=maxc\n",
    "    \n",
    "    #q3.1(d)\n",
    "    clf=select_classifier(c=selected_C)\n",
    "    clf.fit(X_train)\n",
    "    y_pred = clf.decision_function(X_test)\n",
    "    auroc_score=performance(y_test, y_pred, metric=\"auroc\")\n",
    "    print(\"q3.1(d) Choose C which maximizes AUROC\")\n",
    "    print(\"The AUROC score is:\",auroc_score)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    for me in metrics:\n",
    "        if me!=\"auroc\":\n",
    "            score=performance(y_test, y_pred, metric=me)\n",
    "            print(\"The\",me,\"score is\",score)\n",
    "\n",
    "    #q3.1(e)\n",
    "    plot_weight(X_train, y_train, \"l2\", C_range=C_range)\n",
    "\n",
    "    #q3.1(f)\n",
    "    clf = select_classifier(c=0.1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    arg=clf.coef_.argsort()\n",
    "    min_ind4=arg[:4]\n",
    "    max_ind4=arg[:-5:-1]\n",
    "    minwords=[]\n",
    "    maxwords=[]\n",
    "    \n",
    "    for ind in min_ind4:\n",
    "        for word, index in dictionary_binary.items():\n",
    "            if index==ind:\n",
    "                minwords.append(word)\n",
    "    print(\"Most negative words\")\n",
    "    for i in range(4):\n",
    "        print(clf.coef_[min_ind4[i]], minwords[i])\n",
    "    \n",
    "    \n",
    "    for ind in max_ind4:\n",
    "        for word, index in dictionary_binary.items():\n",
    "            if index==ind:\n",
    "                maxwords.append(word)\n",
    "    print(\"Most positive words\")\n",
    "    for i in range(4):\n",
    "        print(clf.coef_[max_ind4[i]], maxwords[i])\n",
    "    \n",
    "    #q3.2(a)\n",
    "    r_range=[1e-3, 1e-2, 0.1, 1, 10, 100, 1000]\n",
    "    cr_range=[]\n",
    "    for c in C_range:\n",
    "        for r in r_range:\n",
    "            cr_range.append([c,r])\n",
    "    [maxc,maxr]=select_param_quadratic(X_train, y_train, param_range=cr_range)\n",
    "    print(\"q3.2(a)\")\n",
    "    print(\"C=\",maxc,\"r=\",maxr,\"is optimal\")\n",
    "\n",
    "    #q3.2(b)\n",
    "    cr_range=[]\n",
    "    for i in range(25):\n",
    "        lgc=random.uniform(-3,3)\n",
    "        lgr=random.uniform(-3,3)\n",
    "        cr_range.append([10**lgc, 10**lgr])\n",
    "    [maxc,maxr]=select_param_quadratic(X_train, y_train, param_range=cr_range)\n",
    "    print(\"q3.2(b)\")\n",
    "    print(\"C=\",maxc,\"r=\",maxr,\"is optimal\")\n",
    "    \n",
    "    #q3.4(a)\n",
    "    maxc=0\n",
    "    maxperf=0\n",
    "    for c in C_range:\n",
    "        clf=select_classifier(penalty='l1', c=c)\n",
    "        y_pred=clf.decision_function(X_test)\n",
    "        perf=performance(y_test, y_pred, \"auroc\")\n",
    "        if perf>maxperf:\n",
    "            maxc=c\n",
    "            maxperf=perf\n",
    "    print(\"q3.4(a)\")\n",
    "    print(\"c=\",maxc,\"is optimal\")\n",
    "    \n",
    "    #q3.4(b)\n",
    "    plot_weight(X_train, y_train, \"l1\", C_range=C_range)\n",
    "    \n",
    "    #q4.1(b)\n",
    "    clf = select_classifier(c=0.01, class_weight={-1: 10,1 :1})\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred=clf.decision_function(X_test)\n",
    "    perf=performance(y_test, y_pred, metric=\"auroc\")\n",
    "    print(\"q4.1(b)\")\n",
    "    print(\"The auroc score is:\",perf)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    for me in metrics:\n",
    "        if me!=\"auroc\":\n",
    "            perf=performance(y_test,y_pred,metric=me)\n",
    "            print(\"The\",me,\"score is\",score)\n",
    "    \n",
    "    #q4.2(a)\n",
    "    clf = select_classifier(c=0.01, class_weight={-1: 1,1 :1})\n",
    "    clf.fit(IMB_features, IMB_labels)\n",
    "    y_pred=clf.decision_function(IMB_test_features)\n",
    "    perf=performance(IMB_test_labels, y_pred, metric=\"auroc\")\n",
    "    print(\"q4.2(a)\")\n",
    "    print(\"The auroc score is:\",perf)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    for me in metrics:\n",
    "        if me!=\"auroc\":\n",
    "            perf=performance(IMB_test_labels,y_pred,metric=me)\n",
    "            print(\"The\",me,\"score is\",score)\n",
    "\n",
    "    #q4.3(a) choose auroc\n",
    "    W_range=[-2,-1.5,-1,-0.5,0,0.5,1,1.5,2]\n",
    "    W_range=[10**w for w in W_range]\n",
    "    maxwn=0\n",
    "    maxwp=0\n",
    "    maxperf=0\n",
    "    for Wn in W_range:\n",
    "        for Wp in W_range:\n",
    "            clf = select_classifier(c=1, class_weight={-1: Wn, 1:Wp})\n",
    "            perf=cv_performance(clf, IMB_features, IMB_labels, metric=\"auroc\")\n",
    "            if perf>maxperf:\n",
    "                maxperf=perf\n",
    "                maxwn=Wn\n",
    "                maxwp=Wp\n",
    "    print(\"q4.3(a) Wn=\",maxwn,\"Wp=\",maxwp,\"is optimal\")\n",
    "    print(\"performance is:\",maxperf)\n",
    "\n",
    "    clf = select_classifier(c=1, class_weight={-1: 100, 1:100})\n",
    "    perf = cv_performance(clf,IMB_features,IMB_labels,metric=\"auroc\")\n",
    "                        \n",
    "    #q4.3(b)\n",
    "    print(\"q4.3(b)\")\n",
    "    print(\"The auroc score is\",maxperf)\n",
    "    clf = select_classifier(c=1, class_weight={-1: maxwn, 1:maxwp})\n",
    "    clf.fit(IMB_features, IMB_labels)\n",
    "    y_pred=clf.predict(IMB_test_features)\n",
    "    for me in metrics:\n",
    "        if me != \"auroc\":\n",
    "            perf=performance(IMB_test_labels, y_pred, metric=me)\n",
    "            print(\"The\",me,\"score is\",perf)\n",
    "\n",
    "    #q4.4\n",
    "    y_pred=clf.predict(IMB_test_features)\n",
    "\n",
    "    clf1=select_classifier(c=1, class_weight={-1: 1,1 :1})\n",
    "    clf1.fit(IMB_features, IMB_labels)\n",
    "    y_pred1=clf1.predict(IMB_test_features)\n",
    "    \n",
    "    \n",
    "    # Read multiclass dataange = C_range\n",
    "    # TODO: Question 5: Apply a classifier to heldout features, and then use\n",
    "    #       generate_challenge_labels to print the predicted labels\n",
    "    multiclass_features, multiclass_labels, multiclass_dictionary = get_multiclass_training_data()\n",
    "    heldout_features = get_heldout_reviews(multiclass_dictionary)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, dictionary_binary = get_split_binary_data()\n",
    "IMB_features, IMB_labels = get_imbalanced_data(dictionary_binary)\n",
    "IMB_test_features, IMB_test_labels = get_imbalanced_test(dictionary_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 2850\n",
      "Average number of non-zero features: 15.624\n"
     ]
    }
   ],
   "source": [
    "# q2\n",
    "print(\"Number of unique words:\",len(X_train[0]))\n",
    "print(\"Average number of non-zero features:\",np.sum(X_train)/len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.768\n",
      "0.01 0.812\n",
      "0.1 0.8390000000000001\n",
      "1 0.834\n",
      "10 0.835\n",
      "100 0.835\n",
      "1000 0.835\n",
      "C= 0.1 is optimal under accuracy metric, cv_perf= 0.8390000000000001\n",
      "0.001 0.7884459063404105\n",
      "0.01 0.8144233771178374\n",
      "0.1 0.8377282080627986\n",
      "1 0.8320179567242159\n",
      "10 0.8328799032077583\n",
      "100 0.8328799032077583\n",
      "1000 0.8328799032077583\n",
      "C= 0.1 is optimal under f1-score metric, cv_perf= 0.8377282080627986\n",
      "0.001 0.86514\n",
      "0.01 0.8952600000000001\n",
      "0.1 0.92036\n",
      "1 0.91404\n",
      "10 0.9141\n",
      "100 0.9141\n",
      "1000 0.9141\n",
      "C= 0.1 is optimal under auroc metric, cv_perf= 0.92036\n",
      "0.001 0.7257324624569865\n",
      "0.01 0.8033860667634253\n",
      "0.1 0.8396602879906849\n",
      "1 0.8396442991260834\n",
      "10 0.8412795192518695\n",
      "100 0.8412795192518695\n",
      "1000 0.8412795192518695\n",
      "C= 10 is optimal under precision metric, cv_perf= 0.8412795192518695\n",
      "0.001 0.8640000000000001\n",
      "0.01 0.8260000000000002\n",
      "0.1 0.8380000000000001\n",
      "1 0.826\n",
      "10 0.826\n",
      "100 0.826\n",
      "1000 0.826\n",
      "C= 0.001 is optimal under sensitivity metric, cv_perf= 0.8640000000000001\n",
      "0.001 0.672\n",
      "0.01 0.798\n",
      "0.1 0.8400000000000001\n",
      "1 0.842\n",
      "10 0.844\n",
      "100 0.844\n",
      "1000 0.844\n",
      "C= 10 is optimal under specificity metric, cv_perf= 0.844\n"
     ]
    }
   ],
   "source": [
    "#q3.1(c)\n",
    "mets=[\"accuracy\",\"f1-score\",\"auroc\",\"precision\",\"sensitivity\",\"specificity\"]\n",
    "selected_C=0\n",
    "C_range=[1e-3, 1e-2, 0.1,1,10,100,1000]\n",
    "for me in mets:\n",
    "    maxc=select_param_linear(X_train, Y_train, metric=me, C_range = C_range)\n",
    "    clf=select_classifier(c=maxc)\n",
    "    score=cv_performance(clf, X_train, Y_train, metric=me)\n",
    "    print(\"C=\",maxc,\"is optimal under\",me,\"metric, cv_perf=\",score)\n",
    "    if me==\"accuracy\":\n",
    "        selected_C=maxc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q3.1(d) Choose C which maximizes accuracy\n",
      "The AUROC score is: 0.92055\n",
      "The accuracy score is 0.8325\n",
      "The f1-score score is 0.8295165394402036\n",
      "The precision score is 0.844559585492228\n",
      "The sensitivity score is 0.815\n",
      "The specificity score is 0.85\n"
     ]
    }
   ],
   "source": [
    "#q3.1(d)\n",
    "clf=select_classifier(c=selected_C)\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_pred = clf.decision_function(X_test)\n",
    "auroc_score=performance(Y_test, Y_pred, metric=\"auroc\")\n",
    "print(\"q3.1(d) Choose C which maximizes accuracy\")\n",
    "print(\"The AUROC score is:\",auroc_score)\n",
    "Y_pred = clf.predict(X_test)\n",
    "for me in mets:\n",
    "    if me!=\"auroc\":\n",
    "        score=performance(Y_test, Y_pred, metric=me)\n",
    "        print(\"The\",me,\"score is\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting the number of nonzero entries of the parameter vector as a function of C\n"
     ]
    }
   ],
   "source": [
    "#q3.1(e)\n",
    "plot_weight(X_train, Y_train, \"l2\", C_range=C_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative words\n",
      "-0.6157880744020268 hours\n",
      "-0.5495052637207417 delayed\n",
      "-0.5208214853103377 due\n",
      "-0.5074326295539983 worst\n",
      "Most positive words\n",
      "0.9694530539313565 thanks\n",
      "0.901084078877419 thank\n",
      "0.7654231353985255 great\n",
      "0.5959079712992326 good\n"
     ]
    }
   ],
   "source": [
    "#q3.1(f)\n",
    "clf = select_classifier(c=0.1)\n",
    "clf.fit(X_train,Y_train)\n",
    "arg=clf.coef_[0].argsort()\n",
    "min_ind4=arg[:4]\n",
    "max_ind4=arg[:-5:-1]\n",
    "minwords=[]\n",
    "maxwords=[]\n",
    "\n",
    "for ind in min_ind4:\n",
    "    for word, index in dictionary_binary.items():\n",
    "        if index==ind:\n",
    "            minwords.append(word)\n",
    "print(\"Most negative words\")\n",
    "for i in range(4):\n",
    "    print(clf.coef_[0,min_ind4[i]], minwords[i])\n",
    "\n",
    "\n",
    "for ind in max_ind4:\n",
    "    for word, index in dictionary_binary.items():\n",
    "        if index==ind:\n",
    "            maxwords.append(word)\n",
    "print(\"Most positive words\")\n",
    "for i in range(4):\n",
    "    print(clf.coef_[0,max_ind4[i]], maxwords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q3.2(a) Grid Search\n",
      "0.001 0.001 0.67\n",
      "0.001 0.01 0.76\n",
      "0.001 0.1 0.77\n",
      "0.001 1 0.768\n",
      "0.001 10 0.768\n",
      "0.001 100 0.768\n",
      "0.001 1000 0.768\n",
      "0.01 0.001 0.67\n",
      "0.01 0.01 0.76\n",
      "0.01 0.1 0.77\n",
      "0.01 1 0.768\n",
      "0.01 10 0.768\n",
      "0.01 100 0.768\n",
      "0.01 1000 0.8\n",
      "0.1 0.001 0.67\n",
      "0.1 0.01 0.76\n",
      "0.1 0.1 0.77\n",
      "0.1 1 0.768\n",
      "0.1 10 0.768\n",
      "0.1 100 0.8\n",
      "0.1 1000 0.8370000000000001\n",
      "1 0.001 0.67\n",
      "1 0.01 0.76\n",
      "1 0.1 0.77\n",
      "1 1 0.768\n",
      "1 10 0.8\n",
      "1 100 0.836\n",
      "1 1000 0.8390000000000001\n",
      "10 0.001 0.67\n",
      "10 0.01 0.76\n",
      "10 0.1 0.77\n",
      "10 1 0.8\n",
      "10 10 0.836\n",
      "10 100 0.8380000000000001\n",
      "10 1000 0.8400000000000001\n",
      "100 0.001 0.67\n",
      "100 0.01 0.76\n",
      "100 0.1 0.8010000000000002\n",
      "100 1 0.836\n",
      "100 10 0.837\n",
      "100 100 0.834\n",
      "100 1000 0.8400000000000001\n",
      "1000 0.001 0.67\n",
      "1000 0.01 0.8019999999999999\n",
      "1000 0.1 0.8370000000000001\n",
      "1000 1 0.837\n",
      "1000 10 0.834\n",
      "1000 100 0.834\n",
      "1000 1000 0.8400000000000001\n",
      "C= 10 r= 1000 is optimal under accuracy metric\n",
      "0.001 0.001 0.742101170374341\n",
      "0.001 0.01 0.7871457857770042\n",
      "0.001 0.1 0.7911080514307887\n",
      "0.001 1 0.7884459063404105\n",
      "0.001 10 0.7884459063404105\n",
      "0.001 100 0.7884459063404105\n",
      "0.001 1000 0.7900826233890139\n",
      "0.01 0.001 0.742101170374341\n",
      "0.01 0.01 0.7871457857770042\n",
      "0.01 0.1 0.7911080514307887\n",
      "0.01 1 0.7884459063404105\n",
      "0.01 10 0.7884459063404105\n",
      "0.01 100 0.7884459063404105\n",
      "0.01 1000 0.8040259084227908\n",
      "0.1 0.001 0.742101170374341\n",
      "0.1 0.01 0.7871457857770042\n",
      "0.1 0.1 0.7911080514307887\n",
      "0.1 1 0.7884459063404105\n",
      "0.1 10 0.7884459063404105\n",
      "0.1 100 0.8040259084227908\n",
      "0.1 1000 0.8352119872144259\n",
      "1 0.001 0.742101170374341\n",
      "1 0.01 0.7871457857770042\n",
      "1 0.1 0.7911080514307887\n",
      "1 1 0.7884459063404105\n",
      "1 10 0.8040259084227908\n",
      "1 100 0.8343679381047284\n",
      "1 1000 0.837124707949663\n",
      "10 0.001 0.742101170374341\n",
      "10 0.01 0.7871457857770042\n",
      "10 0.1 0.7911080514307887\n",
      "10 1 0.8040259084227908\n",
      "10 10 0.8343679381047284\n",
      "10 100 0.8355304180629683\n",
      "10 1000 0.8380890214184824\n",
      "100 0.001 0.742101170374341\n",
      "100 0.01 0.7871457857770042\n",
      "100 0.1 0.8052278196586904\n",
      "100 1 0.8343679381047284\n",
      "100 10 0.8346809412163922\n",
      "100 100 0.8317005974599245\n",
      "100 1000 0.8380890214184824\n",
      "1000 0.001 0.742101170374341\n",
      "1000 0.01 0.807577824314008\n",
      "1000 0.1 0.8356028936253983\n",
      "1000 1 0.8346809412163922\n",
      "1000 10 0.8317005974599245\n",
      "1000 100 0.8317005974599245\n",
      "1000 1000 0.8380890214184824\n",
      "C= 10 r= 1000 is optimal under f1-score metric\n",
      "0.001 0.001 0.86352\n",
      "0.001 0.01 0.8647600000000001\n",
      "0.001 0.1 0.86494\n",
      "0.001 1 0.86494\n",
      "0.001 10 0.86494\n",
      "0.001 100 0.86494\n",
      "0.001 1000 0.86494\n",
      "0.01 0.001 0.86352\n",
      "0.01 0.01 0.8647400000000001\n",
      "0.01 0.1 0.86494\n",
      "0.01 1 0.86494\n",
      "0.01 10 0.86494\n",
      "0.01 100 0.86494\n",
      "0.01 1000 0.88576\n",
      "0.1 0.001 0.86352\n",
      "0.1 0.01 0.86474\n",
      "0.1 0.1 0.86494\n",
      "0.1 1 0.86494\n",
      "0.1 10 0.86494\n",
      "0.1 100 0.88566\n",
      "0.1 1000 0.9175000000000001\n",
      "1 0.001 0.86352\n",
      "1 0.01 0.86474\n",
      "1 0.1 0.86494\n",
      "1 1 0.86494\n",
      "1 10 0.88566\n",
      "1 100 0.9173600000000001\n",
      "1 1000 0.9144399999999999\n",
      "10 0.001 0.86352\n",
      "10 0.01 0.8647199999999999\n",
      "10 0.1 0.86494\n",
      "10 1 0.8856400000000001\n",
      "10 10 0.9173600000000001\n",
      "10 100 0.9143000000000001\n",
      "10 1000 0.9139999999999999\n",
      "100 0.001 0.86352\n",
      "100 0.01 0.86474\n",
      "100 0.1 0.8860199999999999\n",
      "100 1 0.91738\n",
      "100 10 0.9143399999999999\n",
      "100 100 0.9139000000000002\n",
      "100 1000 0.9139999999999999\n",
      "1000 0.001 0.86352\n",
      "1000 0.01 0.88834\n",
      "1000 0.1 0.91776\n",
      "1000 1 0.91422\n",
      "1000 10 0.9141199999999999\n",
      "1000 100 0.9139000000000002\n",
      "1000 1000 0.9139999999999999\n",
      "C= 1000 r= 0.1 is optimal under auroc metric\n",
      "0.001 0.001 0.6103604266508335\n",
      "0.001 0.01 0.7087887639871585\n",
      "0.001 0.1 0.7260878065080889\n",
      "0.001 1 0.7257324624569865\n",
      "0.001 10 0.7257324624569865\n",
      "0.001 100 0.7257324624569865\n",
      "0.001 1000 0.7229265556660559\n",
      "0.01 0.001 0.6103604266508335\n",
      "0.01 0.01 0.7087887639871585\n",
      "0.01 0.1 0.7260878065080889\n",
      "0.01 1 0.7257324624569865\n",
      "0.01 10 0.7257324624569865\n",
      "0.01 100 0.7257324624569865\n",
      "0.01 1000 0.787003409132122\n",
      "0.1 0.001 0.6103604266508335\n",
      "0.1 0.01 0.7087887639871585\n",
      "0.1 0.1 0.7260878065080889\n",
      "0.1 1 0.7257324624569865\n",
      "0.1 10 0.7257324624569865\n",
      "0.1 100 0.787003409132122\n",
      "0.1 1000 0.8402299741013431\n",
      "1 0.001 0.6103604266508335\n",
      "1 0.01 0.7087887639871585\n",
      "1 0.1 0.7260878065080889\n",
      "1 1 0.7257324624569865\n",
      "1 10 0.787003409132122\n",
      "1 100 0.8385446022166418\n",
      "1 1000 0.8443565679571586\n",
      "10 0.001 0.6103604266508335\n",
      "10 0.01 0.7087887639871585\n",
      "10 0.1 0.7260878065080889\n",
      "10 1 0.787003409132122\n",
      "10 10 0.8385446022166418\n",
      "10 100 0.8450234463491268\n",
      "10 1000 0.8459802722299049\n",
      "100 0.001 0.6103604266508335\n",
      "100 0.01 0.7087887639871585\n",
      "100 0.1 0.7874305106658047\n",
      "100 1 0.8385446022166418\n",
      "100 10 0.8432622779642471\n",
      "100 100 0.8409639290982822\n",
      "100 1000 0.8459802722299049\n",
      "1000 0.001 0.6103604266508335\n",
      "1000 0.01 0.7846468153045961\n",
      "1000 0.1 0.838939339058747\n",
      "1000 1 0.8432622779642471\n",
      "1000 10 0.8409639290982822\n",
      "1000 100 0.8409639290982822\n",
      "1000 1000 0.8459802722299049\n",
      "C= 10 r= 1000 is optimal under precision metric\n",
      "0.001 0.001 0.9480000000000001\n",
      "0.001 0.01 0.8859999999999999\n",
      "0.001 0.1 0.8699999999999999\n",
      "0.001 1 0.8640000000000001\n",
      "0.001 10 0.8640000000000001\n",
      "0.001 100 0.8640000000000001\n",
      "0.001 1000 0.8719999999999999\n",
      "0.01 0.001 0.9480000000000001\n",
      "0.01 0.01 0.8859999999999999\n",
      "0.01 0.1 0.8699999999999999\n",
      "0.01 1 0.8640000000000001\n",
      "0.01 10 0.8640000000000001\n",
      "0.01 100 0.8640000000000001\n",
      "0.01 1000 0.8219999999999998\n",
      "0.1 0.001 0.9480000000000001\n",
      "0.1 0.01 0.8859999999999999\n",
      "0.1 0.1 0.8699999999999999\n",
      "0.1 1 0.8640000000000001\n",
      "0.1 10 0.8640000000000001\n",
      "0.1 100 0.8219999999999998\n",
      "0.1 1000 0.8320000000000001\n",
      "1 0.001 0.9480000000000001\n",
      "1 0.01 0.8859999999999999\n",
      "1 0.1 0.8699999999999999\n",
      "1 1 0.8640000000000001\n",
      "1 10 0.8219999999999998\n",
      "1 100 0.8320000000000001\n",
      "1 1000 0.8320000000000001\n",
      "10 0.001 0.9480000000000001\n",
      "10 0.01 0.8859999999999999\n",
      "10 0.1 0.8699999999999999\n",
      "10 1 0.8219999999999998\n",
      "10 10 0.8320000000000001\n",
      "10 100 0.8280000000000001\n",
      "10 1000 0.8320000000000001\n",
      "100 0.001 0.9480000000000001\n",
      "100 0.01 0.8859999999999999\n",
      "100 0.1 0.8240000000000001\n",
      "100 1 0.8320000000000001\n",
      "100 10 0.8280000000000001\n",
      "100 100 0.8240000000000001\n",
      "100 1000 0.8320000000000001\n",
      "1000 0.001 0.9480000000000001\n",
      "1000 0.01 0.8320000000000001\n",
      "1000 0.1 0.834\n",
      "1000 1 0.8280000000000001\n",
      "1000 10 0.8240000000000001\n",
      "1000 100 0.8240000000000001\n",
      "1000 1000 0.8320000000000001\n",
      "C= 0.001 r= 0.001 is optimal under sensitivity metric\n",
      "0.001 0.001 0.392\n",
      "0.001 0.01 0.634\n",
      "0.001 0.1 0.67\n",
      "0.001 1 0.672\n",
      "0.001 10 0.672\n",
      "0.001 100 0.672\n",
      "0.001 1000 0.664\n",
      "0.01 0.001 0.392\n",
      "0.01 0.01 0.634\n",
      "0.01 0.1 0.67\n",
      "0.01 1 0.672\n",
      "0.01 10 0.672\n",
      "0.01 100 0.672\n",
      "0.01 1000 0.7779999999999999\n",
      "0.1 0.001 0.392\n",
      "0.1 0.01 0.634\n",
      "0.1 0.1 0.67\n",
      "0.1 1 0.672\n",
      "0.1 10 0.672\n",
      "0.1 100 0.7779999999999999\n",
      "0.1 1000 0.842\n",
      "1 0.001 0.392\n",
      "1 0.01 0.634\n",
      "1 0.1 0.67\n",
      "1 1 0.672\n",
      "1 10 0.7779999999999999\n",
      "1 100 0.8400000000000001\n",
      "1 1000 0.8460000000000001\n",
      "10 0.001 0.392\n",
      "10 0.01 0.634\n",
      "10 0.1 0.67\n",
      "10 1 0.7779999999999999\n",
      "10 10 0.8400000000000001\n",
      "10 100 0.8480000000000001\n",
      "10 1000 0.8480000000000001\n",
      "100 0.001 0.392\n",
      "100 0.01 0.634\n",
      "100 0.1 0.7779999999999999\n",
      "100 1 0.8400000000000001\n",
      "100 10 0.8459999999999999\n",
      "100 100 0.844\n",
      "100 1000 0.8480000000000001\n",
      "1000 0.001 0.392\n",
      "1000 0.01 0.7719999999999999\n",
      "1000 0.1 0.8400000000000001\n",
      "1000 1 0.8459999999999999\n",
      "1000 10 0.844\n",
      "1000 100 0.844\n",
      "1000 1000 0.8480000000000001\n",
      "C= 10 r= 100 is optimal under specificity metric\n"
     ]
    }
   ],
   "source": [
    "#q3.2(a)\n",
    "r_range=[1e-3, 1e-2, 0.1, 1, 10, 100, 1000]\n",
    "cr_range=[]\n",
    "for c in C_range:\n",
    "    for r in r_range:\n",
    "        cr_range.append([c,r])\n",
    "print(\"q3.2(a) Grid Search\")\n",
    "for me in mets:\n",
    "    [maxc,maxr]=select_param_quadratic(X_train, Y_train, metric=me, param_range=cr_range)\n",
    "    print(\"C=\",maxc,\"r=\",maxr,\"is optimal under\",me,\"metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q3.2(b) Random Search\n",
      "0.010800978176610547 0.038930463835495865 0.768\n",
      "3.075205102240468 0.00459560332947244 0.743\n",
      "0.001310261673541699 0.03918293573305318 0.767\n",
      "0.011839321884196329 245.20063870717846 0.768\n",
      "0.01261901156206203 67.02011909885874 0.768\n",
      "0.001351968038115128 0.0033389892681679792 0.73\n",
      "0.004544379301579318 86.4004014956597 0.768\n",
      "0.12365416930410773 0.10252385807711327 0.77\n",
      "252.39959928508537 62.67761419279087 0.836\n",
      "542.452121299988 56.805331823068435 0.834\n",
      "1.3956069080439013 0.014252550201162464 0.7630000000000001\n",
      "43.87350333678882 664.6273726874244 0.833\n",
      "0.9650310095695557 0.031324366644701084 0.767\n",
      "0.12537884929439447 201.5172366182085 0.821\n",
      "1.4697387410068128 32.524183915944455 0.827\n",
      "150.55571119388117 5.540871261210721 0.845\n",
      "0.20625182072536133 30.541491682883315 0.788\n",
      "0.2849297894450433 2.1458866738438567 0.768\n",
      "1.0071709316137765 0.1800046734774474 0.768\n",
      "0.0027419389387931163 0.0033585318649289528 0.73\n",
      "0.00944526020320772 44.90628983084126 0.768\n",
      "136.61011888990802 5.077127590010835 0.8480000000000001\n",
      "0.07428914572701527 11.997305765835653 0.768\n",
      "12.468892459479347 0.01626538087890011 0.766\n",
      "0.00887397792349072 57.52364826760443 0.768\n",
      "C= 136.61011888990802 r= 5.077127590010835 is optimal under accuracy metric\n",
      "0.010800978176610547 0.038930463835495865 0.7905084819468599\n",
      "3.075205102240468 0.00459560332947244 0.7783773573892582\n",
      "0.001310261673541699 0.03918293573305318 0.7893915278887291\n",
      "0.011839321884196329 245.20063870717846 0.7884459063404105\n",
      "0.01261901156206203 67.02011909885874 0.7884459063404105\n",
      "0.001351968038115128 0.0033389892681679792 0.7730738864949888\n",
      "0.004544379301579318 86.4004014956597 0.7884459063404105\n",
      "0.12365416930410773 0.10252385807711327 0.7911080514307887\n",
      "252.39959928508537 62.67761419279087 0.8340473566365183\n",
      "542.452121299988 56.805331823068435 0.8317005974599245\n",
      "1.3956069080439013 0.014252550201162464 0.7884816243352966\n",
      "43.87350333678882 664.6273726874244 0.8308386509763821\n",
      "0.9650310095695557 0.031324366644701084 0.7898216965067111\n",
      "0.12537884929439447 201.5172366182085 0.823224657597792\n",
      "1.4697387410068128 32.524183915944455 0.8246744907455661\n",
      "150.55571119388117 5.540871261210721 0.843279894943378\n",
      "0.20625182072536133 30.541491682883315 0.791151015272662\n",
      "0.2849297894450433 2.1458866738438567 0.7884459063404105\n",
      "1.0071709316137765 0.1800046734774474 0.7889093189584593\n",
      "0.0027419389387931163 0.0033585318649289528 0.7730738864949888\n",
      "0.00944526020320772 44.90628983084126 0.7884459063404105\n",
      "136.61011888990802 5.077127590010835 0.8462441868394466\n",
      "0.07428914572701527 11.997305765835653 0.7884459063404105\n",
      "12.468892459479347 0.01626538087890011 0.7905167120545948\n",
      "0.00887397792349072 57.52364826760443 0.7884459063404105\n",
      "C= 136.61011888990802 r= 5.077127590010835 is optimal under f1-score metric\n",
      "0.010800978176610547 0.038930463835495865 0.86494\n",
      "3.075205102240468 0.00459560332947244 0.8645400000000001\n",
      "0.001310261673541699 0.03918293573305318 0.86494\n",
      "0.011839321884196329 245.20063870717846 0.86494\n",
      "0.01261901156206203 67.02011909885874 0.86494\n",
      "0.001351968038115128 0.0033389892681679792 0.8644000000000001\n",
      "0.004544379301579318 86.4004014956597 0.86494\n",
      "0.12365416930410773 0.10252385807711327 0.86494\n",
      "252.39959928508537 62.67761419279087 0.91422\n",
      "542.452121299988 56.805331823068435 0.9139399999999999\n",
      "1.3956069080439013 0.014252550201162464 0.8647600000000001\n",
      "43.87350333678882 664.6273726874244 0.9138999999999999\n",
      "0.9650310095695557 0.031324366644701084 0.86484\n",
      "0.12537884929439447 201.5172366182085 0.90466\n",
      "1.4697387410068128 32.524183915944455 0.91116\n",
      "150.55571119388117 5.540871261210721 0.9153800000000001\n",
      "0.20625182072536133 30.541491682883315 0.87432\n",
      "0.2849297894450433 2.1458866738438567 0.86494\n",
      "1.0071709316137765 0.1800046734774474 0.86494\n",
      "0.0027419389387931163 0.0033585318649289528 0.8644000000000001\n",
      "0.00944526020320772 44.90628983084126 0.86494\n",
      "136.61011888990802 5.077127590010835 0.9158799999999999\n",
      "0.07428914572701527 11.997305765835653 0.86494\n",
      "12.468892459479347 0.01626538087890011 0.8648\n",
      "0.00887397792349072 57.52364826760443 0.86494\n",
      "C= 136.61011888990802 r= 5.077127590010835 is optimal under auroc metric\n",
      "0.010800978176610547 0.038930463835495865 0.7223425714366968\n",
      "3.075205102240468 0.00459560332947244 0.6848743336548162\n",
      "0.001310261673541699 0.03918293573305318 0.7218412843482265\n",
      "0.011839321884196329 245.20063870717846 0.7257324624569865\n",
      "0.01261901156206203 67.02011909885874 0.7257324624569865\n",
      "0.001351968038115128 0.0033389892681679792 0.6684418992857555\n",
      "0.004544379301579318 86.4004014956597 0.7257324624569865\n",
      "0.12365416930410773 0.10252385807711327 0.7260878065080889\n",
      "252.39959928508537 62.67761419279087 0.8415887338467984\n",
      "542.452121299988 56.805331823068435 0.8409639290982822\n",
      "1.3956069080439013 0.014252550201162464 0.7136067683235569\n",
      "43.87350333678882 664.6273726874244 0.8393287089724961\n",
      "0.9650310095695557 0.031324366644701084 0.721209691255436\n",
      "0.12537884929439447 201.5172366182085 0.8114985294447813\n",
      "1.4697387410068128 32.524183915944455 0.8349910735993816\n",
      "150.55571119388117 5.540871261210721 0.8484826762246117\n",
      "0.20625182072536133 30.541491682883315 0.7789322269788418\n",
      "0.2849297894450433 2.1458866738438567 0.7257324624569865\n",
      "1.0071709316137765 0.1800046734774474 0.7251205092595047\n",
      "0.0027419389387931163 0.0033585318649289528 0.6684418992857555\n",
      "0.00944526020320772 44.90628983084126 0.7257324624569865\n",
      "136.61011888990802 5.077127590010835 0.8520693139907726\n",
      "0.07428914572701527 11.997305765835653 0.7257324624569865\n",
      "12.468892459479347 0.01626538087890011 0.7168692683235568\n",
      "0.00887397792349072 57.52364826760443 0.7257324624569865\n",
      "C= 136.61011888990802 r= 5.077127590010835 is optimal under precision metric\n",
      "0.010800978176610547 0.038930463835495865 0.874\n",
      "3.075205102240468 0.00459560332947244 0.9019999999999999\n",
      "0.001310261673541699 0.03918293573305318 0.8719999999999999\n",
      "0.011839321884196329 245.20063870717846 0.8640000000000001\n",
      "0.01261901156206203 67.02011909885874 0.8640000000000001\n",
      "0.001351968038115128 0.0033389892681679792 0.9179999999999999\n",
      "0.004544379301579318 86.4004014956597 0.8640000000000001\n",
      "0.12365416930410773 0.10252385807711327 0.8699999999999999\n",
      "252.39959928508537 62.67761419279087 0.8280000000000001\n",
      "542.452121299988 56.805331823068435 0.8240000000000001\n",
      "1.3956069080439013 0.014252550201162464 0.882\n",
      "43.87350333678882 664.6273726874244 0.8240000000000001\n",
      "0.9650310095695557 0.031324366644701084 0.874\n",
      "0.12537884929439447 201.5172366182085 0.836\n",
      "1.4697387410068128 32.524183915944455 0.8160000000000001\n",
      "150.55571119388117 5.540871261210721 0.8400000000000001\n",
      "0.20625182072536133 30.541491682883315 0.8039999999999999\n",
      "0.2849297894450433 2.1458866738438567 0.8640000000000001\n",
      "1.0071709316137765 0.1800046734774474 0.866\n",
      "0.0027419389387931163 0.0033585318649289528 0.9179999999999999\n",
      "0.00944526020320772 44.90628983084126 0.8640000000000001\n",
      "136.61011888990802 5.077127590010835 0.842\n",
      "0.07428914572701527 11.997305765835653 0.8640000000000001\n",
      "12.468892459479347 0.01626538087890011 0.882\n",
      "0.00887397792349072 57.52364826760443 0.8640000000000001\n",
      "C= 0.001351968038115128 r= 0.0033389892681679792 is optimal under sensitivity metric\n",
      "0.010800978176610547 0.038930463835495865 0.662\n",
      "3.075205102240468 0.00459560332947244 0.584\n",
      "0.001310261673541699 0.03918293573305318 0.662\n",
      "0.011839321884196329 245.20063870717846 0.672\n",
      "0.01261901156206203 67.02011909885874 0.672\n",
      "0.001351968038115128 0.0033389892681679792 0.542\n",
      "0.004544379301579318 86.4004014956597 0.672\n",
      "0.12365416930410773 0.10252385807711327 0.67\n",
      "252.39959928508537 62.67761419279087 0.844\n",
      "542.452121299988 56.805331823068435 0.844\n",
      "1.3956069080439013 0.014252550201162464 0.6439999999999999\n",
      "43.87350333678882 664.6273726874244 0.842\n",
      "0.9650310095695557 0.031324366644701084 0.6599999999999999\n",
      "0.12537884929439447 201.5172366182085 0.806\n",
      "1.4697387410068128 32.524183915944455 0.8380000000000001\n",
      "150.55571119388117 5.540871261210721 0.85\n",
      "0.20625182072536133 30.541491682883315 0.7719999999999999\n",
      "0.2849297894450433 2.1458866738438567 0.672\n",
      "1.0071709316137765 0.1800046734774474 0.67\n",
      "0.0027419389387931163 0.0033585318649289528 0.542\n",
      "0.00944526020320772 44.90628983084126 0.672\n",
      "136.61011888990802 5.077127590010835 0.8539999999999999\n",
      "0.07428914572701527 11.997305765835653 0.672\n",
      "12.468892459479347 0.01626538087890011 0.6499999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00887397792349072 57.52364826760443 0.672\n",
      "C= 136.61011888990802 r= 5.077127590010835 is optimal under specificity metric\n"
     ]
    }
   ],
   "source": [
    "#q3.2(b)\n",
    "cr_range=[]\n",
    "for i in range(25):\n",
    "    lgc=random.uniform(-3,3)\n",
    "    lgr=random.uniform(-3,3)\n",
    "    cr_range.append([10**lgc, 10**lgr])\n",
    "print(\"q3.2(b) Random Search\")\n",
    "for me in mets:\n",
    "    [maxc,maxr]=select_param_quadratic(X_train, Y_train, metric=me, param_range=cr_range)\n",
    "    print(\"C=\",maxc,\"r=\",maxr,\"is optimal under\",me,\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q3.4(a)\n",
      "c= 100 is optimal, perf= 0.9179999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#q3.4(a)\n",
    "maxc=0\n",
    "maxperf=0\n",
    "for c in C_range:\n",
    "    clf=select_classifier(penalty='l1', c=c)\n",
    "    perf=cv_performance(clf, X_train, Y_train, metric=\"auroc\")\n",
    "    if perf>maxperf:\n",
    "        maxc=c\n",
    "        maxperf=perf\n",
    "print(\"q3.4(a)\")\n",
    "print(\"c=\",maxc,\"is optimal, perf=\",maxperf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting the number of nonzero entries of the parameter vector as a function of C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#q3.4(b)\n",
    "plot_weight(X_train, Y_train, \"l1\", C_range=C_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q4.1(b)\n",
      "The auroc score is: 0.90495\n",
      "The accuracy score is 0.5625\n",
      "The f1-score score is 0.2222222222222222\n",
      "The precision score is 1.0\n",
      "The sensitivity score is 0.125\n",
      "The specificity score is 1.0\n"
     ]
    }
   ],
   "source": [
    "#q4.1(b)\n",
    "clf = select_classifier(c=0.01, class_weight={-1: 10,1 :1})\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred=clf.decision_function(X_test)\n",
    "perf=performance(Y_test, y_pred, metric=\"auroc\")\n",
    "print(\"q4.1(b)\")\n",
    "print(\"The auroc score is:\",perf)\n",
    "y_pred=clf.predict(X_test)\n",
    "for me in mets:\n",
    "    if me!=\"auroc\":\n",
    "        perf=performance(Y_test,y_pred,metric=me)\n",
    "        print(\"The\",me,\"score is\",perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q4.2(a)\n",
      "The auroc score is: 0.9113500000000001\n",
      "The accuracy score is 0.384\n",
      "The f1-score score is 0.3739837398373984\n",
      "The precision score is 1.0\n",
      "The sensitivity score is 0.23\n",
      "The specificity score is 1.0\n"
     ]
    }
   ],
   "source": [
    "#q4.2(a)\n",
    "clf = select_classifier(c=0.01, class_weight={-1: 1,1 :1})\n",
    "clf.fit(IMB_features, IMB_labels)\n",
    "y_pred=clf.decision_function(IMB_test_features)\n",
    "perf=performance(IMB_test_labels, y_pred, metric=\"auroc\")\n",
    "print(\"q4.2(a)\")\n",
    "print(\"The auroc score is:\",perf)\n",
    "y_pred=clf.predict(IMB_test_features)\n",
    "for me in mets:\n",
    "    if me!=\"auroc\":\n",
    "        perf=performance(IMB_test_labels,y_pred,metric=me)\n",
    "        print(\"The\",me,\"score is\",perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.01 0.12444168734491315\n",
      "0.01 0.03162277660168379 0.6955769075927964\n",
      "0.01 0.1 0.5863484980091217\n",
      "0.01 0.31622776601683794 0.5839421822237544\n",
      "0.01 1 0.5839421822237544\n",
      "0.01 3.1622776601683795 0.5839421822237544\n",
      "0.01 10 0.5839421822237544\n",
      "0.01 31.622776601683793 0.5839421822237544\n",
      "0.01 100 0.5839421822237544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03162277660168379 0.01 nan\n",
      "0.03162277660168379 0.03162277660168379 0.6692107408135747\n",
      "0.03162277660168379 0.1 0.7576461645476125\n",
      "0.03162277660168379 0.31622776601683794 0.7078225420071762\n",
      "0.03162277660168379 1 0.702876571141277\n",
      "0.03162277660168379 3.1622776601683795 0.7018876086688708\n",
      "0.03162277660168379 10 0.7018876086688708\n",
      "0.03162277660168379 31.622776601683793 0.7018876086688708\n",
      "0.03162277660168379 100 0.7018876086688708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.01 nan\n",
      "0.1 0.03162277660168379 0.47917599641737574\n",
      "0.1 0.1 0.7582344932138735\n",
      "0.1 0.31622776601683794 0.7634513837876875\n",
      "0.1 1 0.738260899902735\n",
      "0.1 3.1622776601683795 0.7369627737733225\n",
      "0.1 10 0.7372682599239536\n",
      "0.1 31.622776601683793 0.7372682599239536\n",
      "0.1 100 0.7372682599239536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31622776601683794 0.01 nan\n",
      "0.31622776601683794 0.03162277660168379 0.37672514619883046\n",
      "0.31622776601683794 0.1 0.720474239218763\n",
      "0.31622776601683794 0.31622776601683794 0.7643449470883098\n",
      "0.31622776601683794 1 0.7575740931617749\n",
      "0.31622776601683794 3.1622776601683795 0.7671255664400907\n",
      "0.31622776601683794 10 0.7562566418988372\n",
      "0.31622776601683794 31.622776601683793 0.7562566418988372\n",
      "0.31622776601683794 100 0.7562566418988372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.01 nan\n",
      "1 0.03162277660168379 0.34090556560611673\n",
      "1 0.1 0.678314889788574\n",
      "1 0.31622776601683794 0.7365457345834413\n",
      "1 1 0.7293102762274195\n",
      "1 3.1622776601683795 0.7313549923711752\n",
      "1 10 0.7378283881138952\n",
      "1 31.622776601683793 0.7442457763713352\n",
      "1 100 0.7442457763713352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1622776601683795 0.01 nan\n",
      "3.1622776601683795 0.03162277660168379 0.3318965565971077\n",
      "3.1622776601683795 0.1 0.6352621636169767\n",
      "3.1622776601683795 0.31622776601683794 0.7304727981494143\n",
      "3.1622776601683795 1 0.7199398679526477\n",
      "3.1622776601683795 3.1622776601683795 0.7180519290433057\n",
      "3.1622776601683795 10 0.7129545895953016\n",
      "3.1622776601683795 31.622776601683793 0.7151609196201226\n",
      "3.1622776601683795 100 0.7151609196201226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.01 nan\n",
      "10 0.03162277660168379 0.3318965565971077\n",
      "10 0.1 0.6317790464350368\n",
      "10 0.31622776601683794 0.7126835514807884\n",
      "10 1 0.718609379251981\n",
      "10 3.1622776601683795 0.7055980295505987\n",
      "10 10 0.7079985538644905\n",
      "10 31.622776601683793 0.7104245743563039\n",
      "10 100 0.7104245743563039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.622776601683793 0.01 nan\n",
      "31.622776601683793 0.03162277660168379 0.3318965565971077\n",
      "31.622776601683793 0.1 0.6317790464350368\n",
      "31.622776601683793 0.31622776601683794 0.7126835514807884\n",
      "31.622776601683793 1 0.7186103386899254\n",
      "31.622776601683793 3.1622776601683795 0.7150569040094732\n",
      "31.622776601683793 10 0.7073388937694689\n",
      "31.622776601683793 31.622776601683793 0.6995274106950183\n",
      "31.622776601683793 100 0.6857273450405803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/williamdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:378: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.01 nan\n",
      "100 0.03162277660168379 0.3318965565971077\n",
      "100 0.1 0.6317790464350368\n",
      "100 0.31622776601683794 0.7126835514807884\n",
      "100 1 0.7186103386899254\n",
      "100 3.1622776601683795 0.7150569040094732\n",
      "100 10 0.7073388937694689\n",
      "100 31.622776601683793 0.7017592034224129\n",
      "100 100 0.6995274106950183\n",
      "q4.3(a) Wn= 0.31622776601683794 Wp= 3.1622776601683795 is optimal\n",
      "performance is: 0.7671255664400907\n"
     ]
    }
   ],
   "source": [
    "#q4.3(a) choose f1-score\n",
    "#Phase1: Grid Search\n",
    "W_range=[-2,-1.5,-1,-0.5,0,0.5,1,1.5,2]\n",
    "W_range=[10**w for w in W_range]\n",
    "maxwn=0\n",
    "maxwp=0\n",
    "maxperf=0\n",
    "for Wn in W_range:\n",
    "    for Wp in W_range:\n",
    "        clf = select_classifier(c=1, class_weight={-1: Wn, 1:Wp})\n",
    "        perf=cv_performance(clf, IMB_features, IMB_labels, metric=\"f1-score\")\n",
    "        print(Wn, Wp, perf)\n",
    "        if perf>maxperf:\n",
    "            maxperf=perf\n",
    "            maxwn=Wn\n",
    "            maxwp=Wp\n",
    "print(\"q4.3(a) Wn=\",maxwn,\"Wp=\",maxwp,\"is optimal\")\n",
    "print(\"performance is:\",maxperf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.226470304777975 15.679410914333925 0.7130831913184854\n",
      "3.369505793137678 10.108517379413033 0.7138415075553901\n",
      "5.560072399618675 16.680217198856024 0.716603004838299\n",
      "3.803458691312655 11.410376073937964 0.706000264680322\n",
      "3.2543065706300527 9.762919711890158 0.7138415075553901\n",
      "2.8219237676469873 8.465771302940961 0.7140050021370761\n",
      "0.7097992337704335 2.1293977013113006 0.7411438176024701\n",
      "1.8086042605576678 5.425812781673003 0.7269847285314853\n",
      "1.247388733418862 3.742166200256586 0.7319810632618273\n",
      "3.6197960052877485 10.859388015863246 0.7101903919367288\n",
      "0.2153351617542927 0.6460054852628782 0.7759205408472271\n",
      "0.21214604791131222 0.6364381437339367 0.7773352181239733\n",
      "0.138749925563349 0.41624977669004704 0.7683602959068156\n",
      "0.2940824870685482 0.8822474612056447 0.7653113801452786\n",
      "0.43777680997073654 1.3133304299122095 0.7512510445078595\n",
      "1.172678515678381 3.5180355470351428 0.7332558713162964\n",
      "0.10667557222558927 0.3200267166767678 0.7647375383430113\n",
      "2.0418674846900533 6.12560245407016 0.7227006750389846\n",
      "1.4543275918678926 4.362982775603678 0.7315647559836943\n",
      "0.1883048833325017 0.5649146499975051 0.7733681069960928\n",
      "0.4972613984918912 1.4917841954756736 0.7489427188816824\n",
      "2.5599409468020897 7.679822840406269 0.7250593914259482\n",
      "1.0323624466393648 3.0970873399180947 0.7291872644210329\n",
      "3.774544183884547 11.32363255165364 0.7070148988266635\n",
      "7.999818159021742 23.999454477065225 0.7154392881503937\n",
      "Random Search Wn= 0.21214604791131222 Wp= 0.6364381437339367 is optimal\n",
      "performance is: 0.7773352181239733\n"
     ]
    }
   ],
   "source": [
    "#Phase2: Random Search\n",
    "for i in range(25):\n",
    "    Wn=10**random.uniform(-1,1)\n",
    "    Wp=Wn*3\n",
    "    clf = select_classifier(c=1, class_weight={-1: Wn, 1:Wp})\n",
    "    perf = cv_performance(clf, IMB_features, IMB_labels, metric=\"f1-score\")\n",
    "    print(Wn, Wp, perf)\n",
    "    if perf>maxperf:\n",
    "        maxperf=perf\n",
    "        maxwn=Wn\n",
    "        maxwp=Wp\n",
    "print(\"Random Search Wn=\",maxwn,\"Wp=\",maxwp,\"is optimal\")\n",
    "print(\"performance is:\",maxperf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q4.3(b)\n",
      "The accuracy score is 0.858\n",
      "The f1-score score is 0.9052069425901201\n",
      "The precision score is 0.9713467048710601\n",
      "The sensitivity score is 0.8475\n",
      "The specificity score is 0.9\n",
      "The auroc score is 0.938925\n"
     ]
    }
   ],
   "source": [
    "#q4.3(b)\n",
    "maxwn=0.21214604791131222\n",
    "maxwp=0.6364381437339367\n",
    "mets=[\"accuracy\",\"f1-score\",\"auroc\",\"precision\",\"sensitivity\",\"specificity\"]\n",
    "print(\"q4.3(b)\")\n",
    "clf = select_classifier(c=1, class_weight={-1: maxwn, 1:maxwp})\n",
    "clf.fit(IMB_features, IMB_labels)\n",
    "y_pred=clf.predict(IMB_test_features)\n",
    "for me in mets:\n",
    "    if me != \"auroc\":\n",
    "        perf=performance(IMB_test_labels, y_pred, metric=me)\n",
    "        print(\"The\",me,\"score is\",perf)\n",
    "y_pred=clf.decision_function(IMB_test_features)\n",
    "perf=performance(IMB_test_labels, y_pred, metric=\"auroc\")\n",
    "print(\"The auroc score is\",perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#q4.4\n",
    "y_pred=clf.decision_function(IMB_test_features)\n",
    "fpr, tpr, _ = metrics.roc_curve(IMB_test_labels, y_pred)\n",
    "#balanced weights\n",
    "clf1 = select_classifier(c=0.01, class_weight={-1: 1, 1:1})\n",
    "clf1.fit(IMB_features, IMB_labels)\n",
    "y_pred1=clf1.decision_function(IMB_test_features)\n",
    "fpr1, tpr1, _ = metrics.roc_curve(IMB_test_labels, y_pred1)\n",
    "perf1=performance(IMB_test_labels, y_pred1, metric=\"auroc\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', label='Custom W_n, W_p (AUROC = %0.2f)' % perf)\n",
    "plt.plot(fpr1, tpr1, color='blue', label='Balanced W_n, W_p (AUROC = %0.2f)' % perf1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
